Isaac's instructions:
https://arc-bio.atlassian.net/wiki/spaces/BF/pages/136609793/How+To+-+Launch+Instance+In+AWS

In a nutshell, 

1) Install arcbio-tools following the instructions given in the above link
2) $ cd ~/arcbio-tools/idp_in_aws/create_instance
3) emacs env.sh
   * Change VOL_SIZE (It is in GB. For RNN,try 100 GB. Currently the total size of my RNN folder is 44 GB.)
   * Change EC2_INSTANCE_SIZE try g3.4xlarge(16 core, 122 GB, $1.14 ph)
     as recommended for RNN by https://aws.amazon.com/ec2/pricing/on-demand/
   
     Alex suggested either p3.2xlarge (8 CPU, 61 GB mem, $3.06 per hour) 
                                      or p2.xlarge (4 CPU, 61 GB mem, $0.9 per hour)
  * Change EC2_INSTANCE_NAME to 'Roy_RNN_%' current date
  
  
  4) To launch an instance $./launch.sh create
  you will get a message 
       -----------------------------------------
          *********    PLEASE NOTE!!!    *********
          To Login:
                    ssh -i ~/.ssh/cloud arc@34.220.51.16
          .
          .
          .
       ----------------------------------------

STORE THE LOGIN COMMAND DISPLAYED IN THE ABOVE MESSAGE FOR FUTURE USE
  
  5) Next go to your instance to upload codes, data and to install libraries.
    
    a) Cp your local training data, codes etc to s3. 
    b) Go to the instance using the stored login command (ssh -i blabla). Go to /DaTA01/. Make a folder there, e.g. 'roy_RNN'.
    c) cp the training data, codes etc from s3 to your instance AT /DATA01/roy_RNN/  using the aws command, 
    like you do in servers.
       
    
    Alternatively (not recommended though since it is good to have a copy on s3), you can load directly from 
    your local machine to the AWS instance:

     (on your local machine) $screen -S cpFolderToAWSInstance

                             $ scp -i ~/.ssh/cloud -r <full-path-of-folder-to-copy> arc<aws-server-ip-address>:/DaTA01/roy_RNN/
                                                   
    c) For RNN, install following:
       * brew (OPTIONAL) (the machine is centos so use : 
                sh -c "$(curl -fsSL https://raw.githubusercontent.com/Linuxbrew/install/master/install.sh)")
                
         Add brew installation to your PATH: 
         $vim ~/.bashrc
         Insert the following: export PATH=/home/linuxbrew/.linuxbrew/bin:$PATH
         exit .bashrc
         $source ~/.bashrc
         
      * Install python3: 
                        $ sudo yum install -y https://centos7.iuscommunity.org/ius-release.rpm
                        $ sudo yum update
                        $ sudo yum install -y python36u python36u-libs python36u-devel python36u-pip
                        To check the version: $python3.6 -V
         
      * Install pip3: $sudo yum install python34-setuptools
                      $sudo easy_install-3.4 pip
                      
      * Install virtualenv: $pip3 install --user virtualenv
      
      * Connect python3 to virtualenv and create a virtualenv called py3env, say: 
        $which python3
        You will get /home/linuxbrew/.linuxbrew/bin/python3
        $virtualenv -p /home/linuxbrew/.linuxbrew/bin/python3 py3env
        
     * Activate py3env: $source py3env/bin/activate
     
     * Now install the other required packages
       i) $pip3 install Tensorflow (This will also install numpy)
       ii) $pip3 install scipy
       iii) $pip3 install scikit-learn
       iv) $pip3 install pandas
       v) $pip3 install keras
       vi) $pip3 install matplotlib
       vii) $pip3 install joblib (since importing from sklearn will be deprecated in sklearn v0.23)
       vii) emacs (optional)
            $sudo yum install epel-release
            $sudo yum install emacs
 
    
 6) When an instance completes the submitted job, COPY ALL RESULTS TO S3. THIS IS A MUST!!!
   E.g. aws s3 cp /DaTA01/roy_RNN/ s3://arcbio.research.com/MONTHLY/roy/NN/TrainingModels_6Dec2019/ --recursive
    
   To copy folders to your local machine, in your local machine run:
   $cd ~/Desktop (if you want to copy to desktop)
   $scp -i ~/.ssh/cloud -r arc@34.220.51.16://DaTA01/<folder_to_copy> .
   
   
 
 7) Now it is safe to delete the instance To delete, run $./launch.sh delete in your local machine from where you had
    created the instance (in this case, first $cd ~/arcbio-tools/idp_in_aws/create_instance, then $./launch.sh delete)
  
 
