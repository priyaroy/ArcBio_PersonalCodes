{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf500
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\froman\fcharset0 Times-Roman;\f2\fnil\fcharset0 Menlo-Regular;
\f3\fmodern\fcharset0 Courier;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red255\green255\blue255;\red0\green0\blue233;
\red255\green255\blue11;\red118\green118\blue118;\red0\green0\blue0;\red118\green118\blue118;\red255\green255\blue255;
\red0\green0\blue0;\red0\green0\blue233;\red42\green55\blue62;\red238\green240\blue241;\red17\green0\blue231;
\red0\green0\blue0;\red255\green255\blue255;\red20\green0\blue236;\red20\green0\blue236;\red19\green0\blue235;
}
{\*\expandedcolortbl;;\cssrgb\c0\c1\c1;\cssrgb\c100000\c100000\c99985;\cssrgb\c0\c0\c93333;
\cssrgb\c100000\c100000\c0;\cssrgb\c53749\c53751\c53683;\cssrgb\c0\c0\c0\c84314;\cssrgb\c53691\c53693\c53692;\cssrgb\c100000\c100000\c100000;
\cssrgb\c0\c0\c0\c4706;\cssrgb\c0\c0\c93333;\cssrgb\c21569\c27843\c30980;\cssrgb\c94510\c95294\c95686;\cssrgb\c9689\c9755\c92705;
\cssrgb\c0\c1\c1;\cssrgb\c100000\c100000\c99971;\cssrgb\c11104\c11369\c94281;\cssrgb\c11104\c11369\c94281;\cssrgb\c10631\c10831\c93756;
}
\margl1440\margr1440\vieww11820\viewh14980\viewkind1
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs36 \cf2 \cb3 June 27, 2019\
\
Recurrent NN:\
\
1) Tokenizer:  to convert text to integers\
\
Example here: {\field{\*\fldinst{HYPERLINK "https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17"}}{\fldrslt 
\f1 \cf4 \cb1 \expnd0\expndtw0\kerning0
\ul \ulc4 https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17}}
\f1 \cf4 \cb1 \expnd0\expndtw0\kerning0
\ul \ulc4 \
\
\pard\pardeftab720\sl380\partightenfactor0

\f2\fs32 \cf5 \cb6 \ulnone MAX_NB_WORDS = 50000\
# Max number of words in each complaint.\
MAX_SEQUENCE_LENGTH = 250\
# This is fixed.\
EMBEDDING_DIM = 100\
tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!"#$%&()*+,-./:;<=>?@[\\]^_`\{|\}~', lower=True)\
tokenizer.fit_on_texts(df['Consumer complaint narrative'].values)\
word_index = tokenizer.word_index\
print('Found %s unique tokens.' % len(word_index)\cf7 \cb8 )\
\
\cb9 \
2) Next, truncate and pad the sequences\
\
\cf5 \cb6 X = tokenizer.texts_to_sequences(df['Consumer complaint narrative'].values)\
X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\
print('Shape of data tensor:', X.shape)\cf7 \cb10 \
\
\
3) Convert the pathogen labels into numbers using one-hot encoding.\
\
4) Split data into training and test sets. \
\
5) Create a model which does the following:\
\
   a) Create an embedded layer: This converts text into vectors and is more efficient than 1-hot encoding. \
\
    Keras Embedding documentation: {\field{\*\fldinst{HYPERLINK "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding"}}{\fldrslt 
\f1\fs36 \cf4 \cb1 \ul \ulc4 https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding}}
\f1\fs24 \cf4 \cb1 \ul \ulc4 \
\

\f2\fs32 \cf7 \cb10 \ulnone    This should be the 1st layer in the model. A very good explanation of embedding and its advantage over 1-hot encoding is given in {\field{\*\fldinst{HYPERLINK "https://towardsdatascience.com/deep-learning-4-embedding-layers-f9a02d55ac12"}}{\fldrslt 
\f1\fs36 \cf11 \cb1 \ul \ulc11 \outl0\strokewidth0 \strokec11 https://towardsdatascience.com/deep-learning-4-embedding-layers-f9a02d55ac12}}
\f1\fs36 \cf11 \cb1 \ul \ulc11 \outl0\strokewidth0 \strokec11 .
\f2 \cf7 \cb10 \ulnone \outl0\strokewidth0 \

\fs32 \
   
\f3\fs36 \cf5 \cb6 \uc0\u8234 model = Sequential()\uc0\u8236 \
\pard\pardeftab720\sl280\partightenfactor0
\cf5 \uc0\u8234 model.add(Embedding(X, Y, Z))\uc0\u8236 \
\
X: size of the vocabulary. It is the max. # words to be used. For a 5-mer, it I 4^5 = 1024. \
\
Y: Dim. of the dense embedding. It is the length of the vector or the # latent factors. \
\
Z: input length. It is the length of the input sequence which is a constant after padding. For us, it will be 75 bp/5 for a 5-mer (I guess).\
  
\fs28 \cf12 \cb13 \
\pard\pardeftab720\sl380\partightenfactor0

\f2\fs32 \cf7 \cb10 Here is an explanation of how embedding is better than 1-hot encoding when you have many \'93words\'94:\
\
Suppose you have a 2-mer and a simple read which goes as AGTCGT. You can essentially consider it as a sentence made with 3 words, AG, TC, GT (because each word is a 2-mer). \
\
Now, for a 2-mer with 4 bases, there are only 4^2 = 16 possible combinations. We can refer to this as our \'93vocabulary\'94 for the \'93words\'94 that we are considering. Each word is represented as an integer, going from 1-16: \
\
AA 1\
AC 2\
AG 3\
AT 4\
\
CA 5\
CC 6\
CG 7\
CT 8 and so on. \
\
So, the integer representation of the sentence [AG, TC, GT] is: [3, 14, 12]\
\
Now, what happens here is that each integer is represented in a 10 dimensional space (each dim. is called a latent factor). \
\
\
   Integer    dim.1  dim.2. dim. 3  dim. 4.    \
      1.        0.1.   0.9.   0.3.    0.5\
      2.        0.9.   0.6.   0.2.    0.0\
      3.        0.0.   0.1.   0.4.    0.7\
      .\
      .\
      .\
      16.       1.0.   1.0.   1.0.    0.9\
\
(The #s for the latent factors are right now arbitrary but as the model gets trained, similar \'93words\'94 get placed in similar regions in the 10 dim. space).\
\
This is equivalent to \cf5 \cb10 16X10\cf7 \cb10  dimensional matrix. \
\
\pard\pardeftab720\sl380\partightenfactor0
\cf5 \cb10 If we were using 1-hot encoding\cf7 \cb10 , we would have needed \cf5 \cb10 16X16\cf7 \cb10  matrix to reprent all \'93words\'94. So, as you can see, as we go to bigger k-mers, embedding becomes computationally more efficient and has an additional advantage of grouping similar words in the n-dimensional space. \
\
\
\
\
Additional notes:\
\
1) Very helpful website to understand LSTM {\field{\*\fldinst{HYPERLINK "https://skymind.ai/wiki/lstm#long"}}{\fldrslt 
\f1\fs36 \cf4 \cb1 \ul \ulc4 https://skymind.ai/wiki/lstm#long}}
\f1\fs36 \cf4 \cb1 \ul \ulc4  \
\
\pard\pardeftab720\sl380\partightenfactor0
\cf2 \ulnone   a) Some precautions. Never make the # parameters in your model > # samples. That leads to over-fitting.\cf2 \ul \ulc14  \
\
2) {\field{\*\fldinst{HYPERLINK "http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/"}}{\fldrslt \cf11 \ulc11 \outl0\strokewidth0 \strokec11 http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/}}
\fs24 \cf11 \ulc11 \outl0\strokewidth0 \strokec11 \

\fs36 \cf15 \cb16 \ulnone \strokec17 Explains embedding and has some cool graphs.\
\
3) {\field{\*\fldinst{HYPERLINK "https://www.tensorflow.org/tutorials/representation/word2vec"}}{\fldrslt \cf11 \cb1 \ul \ulc11 \strokec11 https://www.tensorflow.org/tutorials/representation/word2vec}}\
Example of how embedding clusters the \'93words\'94 that capture some general semantic relationships e.g. gender, verb tense etc. \cf15 \cb16 \outl0\strokewidth0 \
\cf4 \cb1 \ul \ulc4 \
\cf15 \cb16 \ulnone 4) GRU (Gated Recurrent Units) Vs. LSTM (Long Short Term Memory)\
\
a) GRU: advantage: \
\
                Computationally faster since it has lesser gates. \
                 \
b) GRU disadvantage: longer sentences do better with LSTM than with GRU. }