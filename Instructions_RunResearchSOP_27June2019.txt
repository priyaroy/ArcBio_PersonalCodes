# Instructions to set up and run research SOP pipeline
#
# P. Roy, (June 12, 2019)
########################################################################


1) Get access to tars & t1000 servers, and AWS from Tom. 

2) Also ask Tom to give you access to Confluence and Atlassian bitbucket.

3) Install required packages:

Go to confluence page: https://arc-bio.atlassian.net/wiki/spaces/BF/pages/
504922114/Arcbio+Our+Private+Packages to look up instructions for cloning 
the required packages. 

   a) Go to your folder on tars: $ssh tars
      Create ~/.pip/pip.conf file following the instructions 1 and 2 on 
      the confluence page. 

   b) Install pip if it is not already installed.
      $pip install --user --upgrade pip

   c) Now create a virtual env:  
       i) $pip install virtualenv
       ii) $virtualenv venv
      
   d) Start virutalenv: $source venv/bin/activate

   e) Now install the required packages. Note: you only need to install pathogenid. 
      It will install the rest of the packages listed at arcbio-pypi. 
       i) $pip install pathogenid

   f) Install R packages:
       
       i) Make sure you are in your virtual environment where you want to 
          create standard report. 

       ii) $ cd ~
          $ R
       iii) > install.packages("optparse") 
           On prompt, select 59 (for USA)

           > install.packages("tidyr")
           > install.packages("dplyr")
           > install.packages("data.table")
           > install.packages("ggplot2")
           > install.packages("pheatmap")
           >quit() # to exit R. Press n on prompt with "Save workspace
                     image?"
            

4 A) Run ResearchSOP to convert bcl files to Trimmed Fastq:

The main instructions are given at https://arc-bio.atlassian.net/wiki/spaces/BF/
pages/554336257/Running+the+ResearchSOP+rsop in atlassian. But some important 
steps are missing. So, here is the complete set. 

   a) Open your .bashrc file and add the following:
      export ARCIVE_CONFIGURATION=/mnt/bdbStorage01/data/s3.json

   b) Save .bashrc and run it: $source ~/.bashrc

   c) Copy an example-data-folder containing the .bcl files to your home directory 
      for a mock run. Note that these folders are huge (~100GB) so copying will 
      take time (~few hrs). 

       i) To copy, $cp -rp /mnt/bdbStorage01/sbs/170720_NB501788_0051_AHMNKGAFXX 
          Here 170720_NB501788_0051_AHMNKGAFXX is one of the runs/samples. 

         Copy everything except the 'projects' folder from 
         170720_NB501788_0051_AHMNKGAFXX. It contains the output 
         (The fastq files) and we don't need to copy it because we will create 
         it ourselves by running the pipeline. 

   d) Go to your home directory: $cd ~

   e) Before we run the researchSOP, start a screen session because the job will
      take a very long time. 

      $ screen -S RSOP

   f) In this screen session, run researchSOP: 
      $luigi --module pathogenid.researchsop AnalyzeRun --directory /home/roy/
       170817_NB501788_0058_AH3HWGBGX3/ --sample-sheet 
       20170817_RUN058_SampleSheet.csv --AvailableThreads-threads 64 
       --local-scheduler > log_RSOP 2> error_SOP

      Here, 20170817_RUN058_SampleSheet.csv is the name of the csv file which is 
      located inside the 170817_NB501788_0058_AH3HWGBGX3 folder. 

      Note that the bcl to fastq conversion code is in the pathogenid package. 

  g) To detach the screen session: ctrl+A, then press d.

  If you only have a run folder with libraries (also known as samples) 
  containing demultiplexed fastqs and you want to get the trimmed fastqs and 
  the multiQC report, then: 


  A') Go to the Run folder. Create a fake .csv file with the same 'Sample-ID' as 
      the library name (e.g. LIB-01) of your sample, the same sample name as your 
      fastq name up to right before '_S%_R%_XXX.fastq.gz' (e.g. Neg_Control), 
      a fake sample project name. The rest of the columns can be blanks. E.g. here:
----------------------------------------------------------------------------------------------------------------
[Header],,,,,,,,,,,
IEMFileVersion,4,,,,,,,,,,
Investigator Name,To be Edited,,,,,,,,,,
Experiment Name,Dummy Sample Sheet,,,,,,,,,,
Date,42661,,,,,,,,,,
Workflow,GenerateFASTQ,,,,,,,,,,
Application,NextSeq FASTQ Only,,,,,,,,,,
Assay,TruSeq HT,,,,,,,,,,
Description,,,,,,,,,,,
Chemistry,Amplicon,,,,,,,,,,
,,,,,,,,,,,
[Reads],,,,,,,,,,,
76,,,,,,,,,,,
76,,,,,,,,,,,
,,,,,,,,,,,
[Settings],,,,,,,,,,,
Adapter,AGATCGGAAGAGCACACGTCTGAACTCCAGTCA,,,,,,,,,,
AdapterRead2,AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT,,,,,,,,,,
,,,,,,,,,,,
[Data],,,,,,,,,,,
Sample_ID,Sample_Name,Sample_Plate,Sample_Well,I7_Index_ID,index,I5_Index_ID,index2,
Sample_Project,Description,seq_date,run_number
LIB-01,Neg_Control,,,  ,  ,  ,  ,UCL_Samples,  ,  ,1
LIB-02,Neg_Control_Dep,,,  ,  ,  ,  ,UCL_Samples,  ,  ,1
.
.
.
----------------------------------------------------------------------------------------------------------------

  B') Make a DeMultiplexingComplete.txt file containing the text 'Done' 
     (without the inverted commas). 

  C') Create a 'projects' folder inside the Run folder. 
 
  D') Create a folder with the same name as the <Sample_Project> in the .csv 
      file. In this example, it is 'UCL_Samples'.

  E') Move your libraries (LIB-01, LIB-02 etc.) inside the projects/UCL_Samples folder. 

  This way you have created a fake instance to simulate the situation where the RSOP 
  code would have created from the bcl files to get the demultiplexed fastqs. 
  Now, you can run RSOP using Luigi in the conventional way. 
   
  luigi --module pathogenid.researchsop AnalyzeRun --directory /home/roy/<your_run_folder>
  --sample-sheet <your_fake_sample_sheet.csv> --AvailableThreads-threads 64 
  --local-scheduler > log_RSOP 2> error_SOP


    
4 B) To get trimmed fastq files from demultiplexed fastq files instead of the bcl files: 

    a) Start the virtual environment. 

    b) Start a new screen session: screen -S TrimmedFastQs
    
    c) $cd home/roy/standalone_scripts/. Make sure this has a 'logs' folder. 

    d) $./fastqTotrimmedFastq.sh <pathToLibDirs> <sample_name> <date>

     E.g. './fastqTotrimmedFastq_V.sh ~/AssayResearchSamples_14Jun2019/Plasma_Precipitation 
           AssaySamples 19June2019'
    

5) Uploading the trimmed fastQs (post bwa + human removal) to arcive:

   a) Stay in your virtual env. Go to /home/roy/identifyTPxStandardAnalysis.R/

   c) Start a new screen session: $ screen -S TrimmedFastQ_Upload
   
   d) Run the script for uploading the trimmed fastq files to arcive.

     $cd ~/identifyTPxStandardAnalysis.R/logs

     $./cleanup_uploadlog (for cleaning up the old log files)

     $ cd ..
     
      $ emacs luigiMultiFileUpload.sh
      
     Open this code and check that it has the right names for the libraries.
     Some libraries are called 'LI-', some are 'LIB-', some are 'SOS' and so on. So, 
     check the library names of your samples and add the same names to line 3 of 
     luigiMultiFileUpload.sh. 
     
     $stdbuf -oL ./luigiMultiFileUpload.sh /home/roy/170817_NB501788_0058_AH3HWGBGX3/
      projects/HIV_Verification > logs/log_trim_upload 2>logs/error_upload

    Important: If you are on t1000 server, then be sure that luigiMultiFileUpload.sh 
    has '--local-scheduler' at the end of the Luigi command. 
      
     Here, HIV_Verification directory is the dir that contains the library folders LI-*, 
     which in turn have the trimmed fastq files in the fastp/ directories.

   e) Exit the screen session: ctrl+A, then d.

6) Run IDP (For alignment and strain calling) after 5) reaches completion 
   (will take about 30-60 min).

   a) First check on arcive that all trimmed files were uploaded successfully.
      i) $pgsql
      arcive=>select * from samples where select * from samples where arc_case_id 
      like 'TRIMMED%_SeraCare%' limit <#trimmed_files you are uploading+1>;

      If you see the correct # of files uploaded with status 'pending', then it means
      the files were uploaded successfully and are waiting for being analyzed. 
      To exit arcive, => \q

   b) Start a new screen session: $ screen -S IDP

   c) $cd /home/roy/identifyTPxStandardAnalysis.R/logs

   d) $./cleanup_IDP_log, $cd ..
   
   e) $ emacs RunIDP.py
      
     Open this code and check that it has the right names for the libraries.
     Some libraries are called 'LI-', some are 'LIB-', some are 'SOS' and so on. So, 
     check the library names of your samples and add the same names to line 44 of 
     RunIDP.py. 
     
     Additionally, if you uploaded the trimmed fastqs of these libraries to arcive more than
     once, then arcive will have multiple copies of the same libs. Change the query in RunIDP.py
     such that you choose the correct id (arcive id) for your libs.
    
   e) $stdbuf -oL ./RunIDP.py /home/roy/170817_NB501788_0058_AH3HWGBGX3/
      20170817_RUN058_SampleSheet.csv 325 /home/roy/170817_NB501788_0058_AH3HWGBGX3/
      projects/HIV_Verification > logs/log_IDP 2>logs/error_IDP

This will write the logs to a 'log' and the errors to 'error_IDP' files.

Stdbuf -oL makes the outputs and errors go immediately to the log files while the
code runs, instead of buffering everything and printing to the files at 
the end of the run.

Make sure to check the log_IDP file and confirm that the arcive ids match the 
library ids (see 'libRecDirMatch' in the log file).

   d) Exit the screen session. 

7) At the completion of 'Run IDP' (a few hours), aggregate all strain files: 

   a) Start a new screen session: $ screen -S StrainAgg

   b) Create a new folder called Strain_Calls in /home/roy/
      170817_NB501788_0058_AH3HWGBGX3/projects/HIV_Verification/

   c) $cd /home/roy/identifyTPxStandardAnalysis.R/ 
   
   d) $emacs idpReportAddInfo.py
   
     Open this code and check that it has the right names for the libraries.
     Some libraries are called 'LI-', some are 'LIB-', some are 'SOS' and so on. So, 
     check the library names of your samples and add the same names to lines 25 and 56 of 
     idpReportAddInfo.py. 

   e) $stdbuf -oL ./idpReportAddInfo.py /home/roy/170817_NB501788_0058_AH3HWGBGX3/
      20170817_RUN058_SampleSheet.csv /home/roy/170817_NB501788_0058_AH3HWGBGX3/
      projects/HIV_Verification 325 > logs/log_StrainAgg 2> logs/error_StrainAgg

   f) Exit the screen session. 

8) At the completion of the above step, create a standard report: 

   a) Go to your virtualenv and start a new screen called StandardReport.

   b) Create a new directory called Std_Report inside the Strain_Calls directory. E.g. 
       $ mkdir /tars01/roy/Strain_Calls/Std_Report

   c) Creat a mapping file <Date>_run<run_number>_mappingFile.csv inside the 
      Strain_Calls directory. Most of the information to create it is already in the 
      <sample_sheet.csv> but you may need some help from Alain or Vaishnavi to create it.

   d) Make sure that all of the *.tsv files with strain calls are inside the 
      Strain_Calls directory. If not, move them there. 
    
   d) To run:
      $cd identifyTPxStandardAnalysis.R

      $ stdbuf -oL Rscript identifyTPxStandardAnalysis.R -i /home/roy/run295/projects/
       TPx_Probit_18reps_30M_run1/Strain_Calls/  -o /home/roy/run295/projects/
       TPx_Probit_18reps_30M_run1/Strain_Calls/Std_Report -m /home/roy/run295/
       projects/TPx_Probit_18reps_30M_run1/Strain_Calls/20190205_run295_mappingFile.csv
       -r _strainCalls.tsv > /home/roy/identifyTPxStandardAnalysis.R/logs/
      log_StdReport_run295_23June2019 2> /home/roy/identifyTPxStandardAnalysis.R/
      logs/error_StdReport_run295_test_23June2019

This will save the standard report in the /Strain_Calls directory. Move them to the 
Std_Report directory within the Strain_Calls Dir.  

DONE!!


10) To save your data on Amazon cloud S3:

    $ cd ~

    Check whether you have aws set up on your laptop or your home dir on the servers,
     from where you want to upload the data to S3. For this:

    $ aws s3 ls s3://arcbio.research.com/

    You should see something like this: 

    $ aws s3 ls s3://arcbio.research.com/
                           PRE MONTHLY/
                           PRE WEEKLY/
                           PRE YEARLY/

   If yes, you are all set to upload your data to S3. To upload your folder: 
   a) Go to S3 web, arcbio research bucket/<your_name>: https://s3.console.aws.amazon.com/
      s3/buckets/arcbio.research.com/MONTHLY/roy/?region=us-east-2&tab=overview
      
   b) Create a new folder with apropriate name representing your sample. E.g. 
      Brown_Samples_MultiQC_24June2019

   c) To copy your directory to S3:
      $aws s3 cp <path_to_your_folder> s3://arcbio.research.com/MONTHLY/roy/
       Brown_Samples_MultiQC_24June2019 --recursive
      
      If you are uploading a file, remove the '--recursive' option.





--------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------
#Below are some notes on Vaishnavi's way of uploading to arcive, running IDP. I am not 
using this anymore, but it has some useful information about arcive. 

5) 30 min after you run the researchSOP, you can start another screen and upload the pathogens 
(the  to Archive. These are the *_R1_* (paired end 1) and *_R2_* (paired end 2) files 
located in projects/HIV_verification/LI* folders. 

$ screen -S upload2Arcive

$ cd 170720_NB501788_0051_AHMNKGAFXX/projects/HIV_Verification

$ for dir in LI*; do cd $dir; f1=$(find . -maxdepth 1 -name "*_R1_*"); f2=$(find . 
-maxdepth 1 -name "*_R2_*"); luigi --module pathogenid.newdata NewSample 
--paired-end-one $f1 --paired-end-two $f2 --local-scheduler; cd ../; done

Then, detach the screen.

6) To monitor the progress, $htop -u roy

   To see if how many screen sessions are running and their names, $screen -ls

7) The uploading of the fastq files will take about 30 min. You can check whether the 
uploading reached completion by going back to the screen session and checking the log. 
If it says something along the lines 'progress looks :) ...' then it indicates that 
the job finished without a problem. 

Another way to check is by checking how many fastq files are there. The # is the same 
as the # of LI* directories. 

$ls -ld 170817_NB501788_0058_AH3HWGBGX3/projects/HIV_Verification/LI* | wc -l

Then login to the arcive database using postgre sql. The fastq files are uploaded to
 the 'samples' table. I have set up an alias 'pgsql' for the login command.

To login to archive: $pgsql

arcive=> select * from samples order by id desc limit <# of LI folders +1>; # shows 
fastq files uploaded in descending order (i.e. most recent to back in time). 

You should be able to see all of your .fastq files if the uploading completed. 

To exit arcive: arcive=>\q

8) To run identify SOP (i.e. alignment + strain calling on the demultiplexed fastq files):

   a) Go back to the upload2Arcive screen session or start a new screen.

   b) ssh to tars. 

   c) Copy the sample IDs of your fastq files from arcive to a text file in your 
      home folder:

       i) Login to arcive by running $pgsql.

       ii) arcive=> \copy (select id from samples order by id DESC limit 
           <# of LI folders>) to sample.txt

       iii) arcive=> \q

     This will save all sample IDs to a sample.txt file which will be saved in the 
     same directory from where you connected to arcive (which, in this case, is 
     your home folder). 


#######################################################################################
Extra notes
#######################################################################################

1) To change names of files from *.gz to *_new.gz, inside all LI-* directories:/
   Users/priyaroy/Desktop/Instructions_RunResearchSOP_16June2019.txt

$for dir in LI-*; do cd $dir; for file in *.gz; do mv "$file" "New${file%.gz}.gz";
 done; cd ..; done;
   

2) To list all tables in our pgsql information_schema arrive:
arcive=>SELECT table_name FROM information_schema.tables WHERE table_schema='public';

3) To rename multiple files: 

   $ rename 'original_string' replacement_string *.<file_extension>


   E.g. $rename 'Strain_Callsprobit' probit *.tsv 

        Renames all tsk files with names like Strain_Callsprobit_TPx_30M_LIB*.tsv
        to probit_TPx_30M_LIB*.tsv

4) To check the codes inside pathogenic, go to your virtual env directory where you 
Installed pathogenid. You will find it there. For me, the virtual env is RSOP. The
Path to pathogenid is:

/home/roy/RSOP/lib/python2.7/site-packages/pathogenid

After you make any change, do 
#$pip install . -- upgrade 

to save and compile the changes.

 5) To rename all files within multiple directories such that the directory
 Names go before the corresponding file names within those directories: 

E.g. suppose you have LI-01, LI-02, LI-03.

LI-01: 
       1.txt  2.txt
LI-02: 
       1.txt  2.txt
LI-03:
       1.txt  2.txt

You want:

LI-01: 
       LI-01_1.txt  LI-01_2.txt
LI-02: 
       LI-02_1.txt  LI-02_2.txt
LI-03:
       LI-03_1.txt  LI-03_2.txt
     

For this, type in bash:

$for dir in LI-*; do cd $dir; for i in *.txt; do newname="$dir""_""$i"; mv $i $newname; done; cd ../; done

6) To make multiple directories using for loop in bash:

   Eg. if you want to creat LI-15 to LI-28 directories, 
       $for i in `seq 15 28`; do mkdir LI-$i; done;

      







